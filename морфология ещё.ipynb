{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт нужных модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "import json\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import jsonlines\n",
    "import collections as c\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 2\n",
    "Обработать книгу с майстем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "with open ('vody.txt', encoding = 'utf-8') as f:\n",
    "    big_text = f.read()\n",
    "%time lemmas = m.lemmatize(big_text)\n",
    "with open('data.json', 'a', encoding='utf-8') as j:\n",
    "    json.dump(lemmas, j, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 3\n",
    "Обработать книгу с pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 319 ms\n"
     ]
    }
   ],
   "source": [
    "%time tok = word_tokenize(big_text)\n",
    "for w in tok:\n",
    "    p = morph.parse(w)[0]\n",
    "    #my_tag = p.tag.POS\n",
    "    #print(my_tag)\n",
    "    info = {'lemma' : p.normal_form, 'word': p.word, 'pos': p.tag.POS}\n",
    "    #print(info)\n",
    "    with jsonlines.open('output.jsonl', mode='a') as writer:\n",
    "        writer.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 292 ms\n"
     ]
    }
   ],
   "source": [
    "%time tok = word_tokenize(big_text)\n",
    "lemmas = []\n",
    "for w in tok:\n",
    "    p = morph.parse(w)[0]\n",
    "    lemmas.append(list(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 4\n",
    "Какую долю слов составляет каждая часть речи?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-65e6aaa49571>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msumm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmy_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmorph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tok' is not defined"
     ]
    }
   ],
   "source": [
    "pos = c.defaultdict(list)\n",
    "summ = 0\n",
    "my_data = {}\n",
    "for w in tok:\n",
    "    p = morph.parse(w)[0]\n",
    "    part = p.tag.POS\n",
    "    pos[part].append(w)\n",
    "for p in pos.keys():\n",
    "    summ += len(pos[p])\n",
    "for p in pos.keys():\n",
    "    perc = (len(pos[p])/summ)*100\n",
    "    print(p, ' составляет ', perc, '%', sep = '')\n",
    "    my_data[p] = perc\n",
    "df = pd.DataFrame(my_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите топ-20 (по частотности) глаголов и наречий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('было', 118), ('был', 79), ('будет', 42), ('мог', 40), ('была', 39), ('может', 37), ('спросил', 29), ('мой', 26), ('заметил', 25), ('воскликнул', 25), ('сказала', 24), ('промолвил', 22), ('подумал', 20), ('буду', 20), ('хотел', 19), ('взял', 19), ('спросила', 19), ('отвечал', 19), ('промолвила', 18), ('знал', 17)]\n",
      "[('уже', 108), ('только', 92), ('очень', 71), ('теперь', 56), ('опять', 52), ('несколько', 49), ('вдруг', 45), ('раз', 44), ('там', 42), ('наконец', 40), ('тотчас', 38), ('потом', 30), ('всего', 29), ('почти', 28), ('никогда', 28), ('тут', 28), ('назад', 27), ('где', 26), ('слегка', 26), ('вперед', 25)]\n"
     ]
    }
   ],
   "source": [
    "verb = pos['VERB']\n",
    "count_v = c.Counter(verb)\n",
    "print(count_v.most_common(20))\n",
    "adverb = pos['ADVB']\n",
    "count_a = c.Counter(adverb)\n",
    "print(count_a.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 5\n",
    "топ-25 биграмм и триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('марья', 'николаевна'), 118), (('фрау', 'леноре'), 78), (('что', 'он'), 67), (('и', 'не'), 52), (('что', 'я'), 44), (('я', 'не'), 42), (('на', 'него'), 42), (('с', 'ним'), 42), (('он', 'не'), 41), (('к', 'нему'), 31), (('что', 'вы'), 30), (('и', 'он'), 28), (('и', 'даже'), 27), (('в', 'нем'), 25), (('у', 'меня'), 25), (('как', 'бы'), 24), (('и', 'с'), 24), (('не', 'мог'), 22), (('так', 'и'), 22), (('и', 'в'), 21), (('что', 'она'), 21), (('с', 'вами'), 21), (('я', 'вас'), 20), (('то', 'что'), 20), (('ничего', 'не'), 19)] [(('в', 'первый', 'раз'), 10), (('что', 'он', 'не'), 8), (('к', 'нему', 'и'), 8), (('на', 'него', 'и'), 8), (('марья', 'николаевна', 'и'), 8), (('к', 'санину', 'и'), 7), (('с', 'ним', 'в'), 6), (('на', 'следующий', 'день'), 6), (('в', 'комнату', 'и'), 6), (('то', 'же', 'время'), 6), (('а', 'марья', 'николаевна'), 6), (('со', 'всех', 'сторон'), 5), (('в', 'то', 'же'), 5), (('и', 'как', 'бы'), 5), (('по', 'крайней', 'мере'), 5), (('с', 'своей', 'стороны'), 5), (('но', 'так', 'как'), 5), (('в', 'свою', 'очередь'), 5), (('и', 'я', 'не'), 5), (('у', 'него', 'в'), 5), (('не', 'правда', 'ли'), 5), (('в', 'то', 'время'), 5), (('с', 'нею', 'и'), 5), (('у', 'нас', 'в'), 5), (('за', 'то', 'что'), 5)]\n"
     ]
    }
   ],
   "source": [
    "words = [w.lower() for w in tok if w.isalpha()]\n",
    "wordsbigrm = nltk.bigrams(words)\n",
    "bg = list(wordsbigrm)\n",
    "cb = c.Counter(bg)\n",
    "wordstrigrm = nltk.trigrams(words)\n",
    "tg = list(wordstrigrm)\n",
    "ct = c.Counter(tg)\n",
    "print(cb.most_common(25), ct.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
